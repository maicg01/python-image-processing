{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link tham khao: https://viblo.asia/p/ong-toan-vi-loc-bi-kip-vo-cong-de-tao-mo-hinh-sieu-sieu-nho-li-ti-voi-do-chinh-xac-khong-lo-Qpmleon9Krd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maicg/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xay dung base model\n",
    "class PruningModule(Module):\n",
    "    def prune_by_std(self, s=0.25): # tuy chinh tham so s=0.25 de tinh toan gia tri cua threshold can cat tia, 25% cá»§a average standard deviation: gia tri do lech chuan trung binh\n",
    "        # Note that module here is the layer\n",
    "        # ex) fc1, fc2, fc3\n",
    "        for name, module in self.named_modules():\n",
    "            if name in ['fc1', 'fc2', 'fc3']:\n",
    "                threshold = np.std(module.weight.data.cpu().numpy()) * s\n",
    "                print(f'Pruning with threshold : {threshold} for layer {name}')\n",
    "                module.prune(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xay dung model cat tia\n",
    "class MaskedLinear(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(MaskedLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        # Initialize the mask with 1\n",
    "        self.mask = Parameter(torch.ones([out_features, in_features]), requires_grad=False) # mat na duoc khoi tao la 1. Sau cat tia nhung weight nao khong can nua se thanh 0\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Caculate weight when forward step\n",
    "        return F.linear(input, self.weight * self.mask, self.bias) # thay vi nhan thang voi input, nhan voi bo loc truoc, giup loai bo di cac weight khong can thiet\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'in_features=' + str(self.in_features) \\\n",
    "            + ', out_features=' + str(self.out_features) \\\n",
    "            + ', bias=' + str(self.bias is not None) + ')'\n",
    "\n",
    "    # Customize of prune function with mask\n",
    "    def prune(self, threshold): #tinh toan lai cac trong so co weight nho hon nguong quy dinh, cap nhat lai mask va weight tai cac vi tri do ve gia tri 0\n",
    "        weight_dev = self.weight.device\n",
    "        mask_dev = self.mask.device\n",
    "        # Convert Tensors to numpy and calculate\n",
    "        tensor = self.weight.data.cpu().numpy()\n",
    "        mask = self.mask.data.cpu().numpy()\n",
    "        new_mask = np.where(abs(tensor) < threshold, 0, mask)\n",
    "        # Apply new weight and mask\n",
    "        self.weight.data = torch.from_numpy(tensor * new_mask).to(weight_dev)\n",
    "        self.mask.data = torch.from_numpy(new_mask).to(mask_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cai dat mang Fully Connected\n",
    "class LeNet(PruningModule):\n",
    "    def __init__(self, mask=False):\n",
    "        super(LeNet, self).__init__()\n",
    "        linear = MaskedLinear if mask else nn.Linear\n",
    "        self.fc1 = linear(784, 300)\n",
    "        self.fc2 = linear(300, 100)\n",
    "        self.fc3 = linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cai dat mot so hyperparemeter\n",
    "# Define some const\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "USE_CUDA = True\n",
    "SEED = 42\n",
    "LOG_AFTER = 10 # How many batches to wait before logging training status\n",
    "LOG_FILE = 'log_prunting.txt'\n",
    "SENSITIVITY = 2 # Sensitivity value that is multiplied to layer's std in order to get threshold value\n",
    "\n",
    "# Control Seed\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Select Device\n",
    "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344it [00:01, 7245226.28it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 23112646.42it/s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1649664it [00:00, 10129580.53it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5120it [00:00, 7013336.54it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cai dat dataloader\n",
    "# Create the dataset with MNIST\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Train loader\n",
    "kwargs = {'num_workers': 5, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "# Test loader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet(mask=True).to(device) #dinh nghia model chuan bi cho buoc training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dinh nghia optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define optimizer with Adam function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n",
    "initial_optimizer_state_dict = optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training mo hinh\n",
    "\n",
    "from tqdm import tqdm \n",
    "# Define training function \n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        for batch_idx, (data, target) in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # zero-out all the gradients corresponding to the pruned connections\n",
    "            for name, p in model.named_parameters():\n",
    "                if 'mask' in name:\n",
    "                    continue\n",
    "                tensor = p.data.cpu().numpy()\n",
    "                grad_tensor = p.grad.data.cpu().numpy()\n",
    "                grad_tensor = np.where(tensor==0, 0, grad_tensor)\n",
    "                p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "\n",
    "            optimizer.step()\n",
    "            if batch_idx % LOG_AFTER == 0:\n",
    "                done = batch_idx * len(data)\n",
    "                percentage = 100. * batch_idx / len(train_loader)\n",
    "                pbar.set_description(f'Train Epoch: {epoch} [{done:5}/{len(train_loader.dataset)} ({percentage:3.0f}%)]  Loss: {loss.item():.6f}')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [58880/60000 ( 98%)]  Loss: 0.194420: 100%|ââââââââââ| 469/469 [00:01<00:00, 274.01it/s]\n",
      "Train Epoch: 1 [58880/60000 ( 98%)]  Loss: 0.165254: 100%|ââââââââââ| 469/469 [00:01<00:00, 285.81it/s]\n",
      "Train Epoch: 2 [58880/60000 ( 98%)]  Loss: 0.104356: 100%|ââââââââââ| 469/469 [00:01<00:00, 291.94it/s]\n",
      "Train Epoch: 3 [58880/60000 ( 98%)]  Loss: 0.051073: 100%|ââââââââââ| 469/469 [00:01<00:00, 290.57it/s]\n",
      "Train Epoch: 4 [58880/60000 ( 98%)]  Loss: 0.009828: 100%|ââââââââââ| 469/469 [00:01<00:00, 288.48it/s]\n",
      "Train Epoch: 5 [58880/60000 ( 98%)]  Loss: 0.115870: 100%|ââââââââââ| 469/469 [00:01<00:00, 288.95it/s]\n",
      "Train Epoch: 6 [58880/60000 ( 98%)]  Loss: 0.059367: 100%|ââââââââââ| 469/469 [00:01<00:00, 300.99it/s]\n",
      "Train Epoch: 7 [58880/60000 ( 98%)]  Loss: 0.049818: 100%|ââââââââââ| 469/469 [00:01<00:00, 277.69it/s]\n",
      "Train Epoch: 8 [58880/60000 ( 98%)]  Loss: 0.009983: 100%|ââââââââââ| 469/469 [00:01<00:00, 290.78it/s]\n",
      "Train Epoch: 9 [58880/60000 ( 98%)]  Loss: 0.007569: 100%|ââââââââââ| 469/469 [00:01<00:00, 288.41it/s]\n",
      "Train Epoch: 10 [58880/60000 ( 98%)]  Loss: 0.010823: 100%|ââââââââââ| 469/469 [00:01<00:00, 298.67it/s]\n",
      "Train Epoch: 11 [58880/60000 ( 98%)]  Loss: 0.011547: 100%|ââââââââââ| 469/469 [00:01<00:00, 282.31it/s]\n",
      "Train Epoch: 12 [58880/60000 ( 98%)]  Loss: 0.006702: 100%|ââââââââââ| 469/469 [00:01<00:00, 291.04it/s]\n",
      "Train Epoch: 13 [58880/60000 ( 98%)]  Loss: 0.041208: 100%|ââââââââââ| 469/469 [00:01<00:00, 284.36it/s]\n",
      "Train Epoch: 14 [58880/60000 ( 98%)]  Loss: 0.011236: 100%|ââââââââââ| 469/469 [00:01<00:00, 291.60it/s]\n",
      "Train Epoch: 15 [58880/60000 ( 98%)]  Loss: 0.014056: 100%|ââââââââââ| 469/469 [00:01<00:00, 291.95it/s]\n",
      "Train Epoch: 16 [58880/60000 ( 98%)]  Loss: 0.048862: 100%|ââââââââââ| 469/469 [00:01<00:00, 278.72it/s]\n",
      "Train Epoch: 17 [58880/60000 ( 98%)]  Loss: 0.004331: 100%|ââââââââââ| 469/469 [00:01<00:00, 277.93it/s]\n",
      "Train Epoch: 18 [58880/60000 ( 98%)]  Loss: 0.015575: 100%|ââââââââââ| 469/469 [00:01<00:00, 283.01it/s]\n",
      "Train Epoch: 19 [58880/60000 ( 98%)]  Loss: 0.013698: 100%|ââââââââââ| 469/469 [00:01<00:00, 286.43it/s]\n",
      "Train Epoch: 20 [58880/60000 ( 98%)]  Loss: 0.073062: 100%|ââââââââââ| 469/469 [00:01<00:00, 247.81it/s]\n",
      "Train Epoch: 21 [58880/60000 ( 98%)]  Loss: 0.008213: 100%|ââââââââââ| 469/469 [00:01<00:00, 275.50it/s]\n",
      "Train Epoch: 22 [58880/60000 ( 98%)]  Loss: 0.001059: 100%|ââââââââââ| 469/469 [00:01<00:00, 263.83it/s]\n",
      "Train Epoch: 23 [58880/60000 ( 98%)]  Loss: 0.011207: 100%|ââââââââââ| 469/469 [00:01<00:00, 266.39it/s]\n",
      "Train Epoch: 24 [58880/60000 ( 98%)]  Loss: 0.017627: 100%|ââââââââââ| 469/469 [00:01<00:00, 253.83it/s]\n",
      "Train Epoch: 25 [58880/60000 ( 98%)]  Loss: 0.024970: 100%|ââââââââââ| 469/469 [00:01<00:00, 264.64it/s]\n",
      "Train Epoch: 26 [58880/60000 ( 98%)]  Loss: 0.050570: 100%|ââââââââââ| 469/469 [00:01<00:00, 270.87it/s]\n",
      "Train Epoch: 27 [58880/60000 ( 98%)]  Loss: 0.001021: 100%|ââââââââââ| 469/469 [00:01<00:00, 271.86it/s]\n",
      "Train Epoch: 28 [58880/60000 ( 98%)]  Loss: 0.009540: 100%|ââââââââââ| 469/469 [00:01<00:00, 251.10it/s]\n",
      "Train Epoch: 29 [58880/60000 ( 98%)]  Loss: 0.050437: 100%|ââââââââââ| 469/469 [00:01<00:00, 276.54it/s]\n",
      "Train Epoch: 30 [58880/60000 ( 98%)]  Loss: 0.003071: 100%|ââââââââââ| 469/469 [00:01<00:00, 268.35it/s]\n",
      "Train Epoch: 31 [58880/60000 ( 98%)]  Loss: 0.072280: 100%|ââââââââââ| 469/469 [00:01<00:00, 273.53it/s]\n",
      "Train Epoch: 32 [58880/60000 ( 98%)]  Loss: 0.011079: 100%|ââââââââââ| 469/469 [00:01<00:00, 261.81it/s]\n",
      "Train Epoch: 33 [58880/60000 ( 98%)]  Loss: 0.045397: 100%|ââââââââââ| 469/469 [00:01<00:00, 283.61it/s]\n",
      "Train Epoch: 34 [58880/60000 ( 98%)]  Loss: 0.023991: 100%|ââââââââââ| 469/469 [00:01<00:00, 267.41it/s]\n",
      "Train Epoch: 35 [58880/60000 ( 98%)]  Loss: 0.003117: 100%|ââââââââââ| 469/469 [00:01<00:00, 276.81it/s]\n",
      "Train Epoch: 36 [58880/60000 ( 98%)]  Loss: 0.000613: 100%|ââââââââââ| 469/469 [00:01<00:00, 273.64it/s]\n",
      "Train Epoch: 37 [58880/60000 ( 98%)]  Loss: 0.010596: 100%|ââââââââââ| 469/469 [00:01<00:00, 272.67it/s]\n",
      "Train Epoch: 38 [58880/60000 ( 98%)]  Loss: 0.001020: 100%|ââââââââââ| 469/469 [00:01<00:00, 268.39it/s]\n",
      "Train Epoch: 39 [58880/60000 ( 98%)]  Loss: 0.008998: 100%|ââââââââââ| 469/469 [00:01<00:00, 270.03it/s]\n",
      "Train Epoch: 40 [58880/60000 ( 98%)]  Loss: 0.017531: 100%|ââââââââââ| 469/469 [00:01<00:00, 264.31it/s]\n",
      "Train Epoch: 41 [58880/60000 ( 98%)]  Loss: 0.023579: 100%|ââââââââââ| 469/469 [00:01<00:00, 275.56it/s]\n",
      "Train Epoch: 42 [58880/60000 ( 98%)]  Loss: 0.000715: 100%|ââââââââââ| 469/469 [00:01<00:00, 272.55it/s]\n",
      "Train Epoch: 43 [58880/60000 ( 98%)]  Loss: 0.001490: 100%|ââââââââââ| 469/469 [00:01<00:00, 283.34it/s]\n",
      "Train Epoch: 44 [58880/60000 ( 98%)]  Loss: 0.003673: 100%|ââââââââââ| 469/469 [00:01<00:00, 273.14it/s]\n",
      "Train Epoch: 45 [58880/60000 ( 98%)]  Loss: 0.022133: 100%|ââââââââââ| 469/469 [00:01<00:00, 280.94it/s]\n",
      "Train Epoch: 46 [58880/60000 ( 98%)]  Loss: 0.006005: 100%|ââââââââââ| 469/469 [00:01<00:00, 280.01it/s]\n",
      "Train Epoch: 47 [58880/60000 ( 98%)]  Loss: 0.001400: 100%|ââââââââââ| 469/469 [00:01<00:00, 273.20it/s]\n",
      "Train Epoch: 48 [58880/60000 ( 98%)]  Loss: 0.000823: 100%|ââââââââââ| 469/469 [00:01<00:00, 275.81it/s]\n",
      "Train Epoch: 49 [58880/60000 ( 98%)]  Loss: 0.001758: 100%|ââââââââââ| 469/469 [00:01<00:00, 269.91it/s]\n",
      "Train Epoch: 50 [58880/60000 ( 98%)]  Loss: 0.002808: 100%|ââââââââââ| 469/469 [00:01<00:00, 269.89it/s]\n",
      "Train Epoch: 51 [58880/60000 ( 98%)]  Loss: 0.020966: 100%|ââââââââââ| 469/469 [00:01<00:00, 273.18it/s]\n",
      "Train Epoch: 52 [58880/60000 ( 98%)]  Loss: 0.001215: 100%|ââââââââââ| 469/469 [00:01<00:00, 276.25it/s]\n",
      "Train Epoch: 53 [58880/60000 ( 98%)]  Loss: 0.023093: 100%|ââââââââââ| 469/469 [00:01<00:00, 269.69it/s]\n",
      "Train Epoch: 54 [58880/60000 ( 98%)]  Loss: 0.000736: 100%|ââââââââââ| 469/469 [00:01<00:00, 279.21it/s]\n",
      "Train Epoch: 55 [58880/60000 ( 98%)]  Loss: 0.002866: 100%|ââââââââââ| 469/469 [00:01<00:00, 263.90it/s]\n",
      "Train Epoch: 56 [58880/60000 ( 98%)]  Loss: 0.017907: 100%|ââââââââââ| 469/469 [00:01<00:00, 260.26it/s]\n",
      "Train Epoch: 57 [58880/60000 ( 98%)]  Loss: 0.009268: 100%|ââââââââââ| 469/469 [00:01<00:00, 280.30it/s]\n",
      "Train Epoch: 58 [58880/60000 ( 98%)]  Loss: 0.019021: 100%|ââââââââââ| 469/469 [00:01<00:00, 262.60it/s]\n",
      "Train Epoch: 59 [58880/60000 ( 98%)]  Loss: 0.025339: 100%|ââââââââââ| 469/469 [00:01<00:00, 277.64it/s]\n",
      "Train Epoch: 60 [58880/60000 ( 98%)]  Loss: 0.003841: 100%|ââââââââââ| 469/469 [00:01<00:00, 263.95it/s]\n",
      "Train Epoch: 61 [58880/60000 ( 98%)]  Loss: 0.027699: 100%|ââââââââââ| 469/469 [00:01<00:00, 273.67it/s]\n",
      "Train Epoch: 62 [58880/60000 ( 98%)]  Loss: 0.011382: 100%|ââââââââââ| 469/469 [00:01<00:00, 276.63it/s]\n",
      "Train Epoch: 63 [58880/60000 ( 98%)]  Loss: 0.010203: 100%|ââââââââââ| 469/469 [00:01<00:00, 267.91it/s]\n",
      "Train Epoch: 64 [58880/60000 ( 98%)]  Loss: 0.010644: 100%|ââââââââââ| 469/469 [00:01<00:00, 285.63it/s]\n",
      "Train Epoch: 65 [58880/60000 ( 98%)]  Loss: 0.002959: 100%|ââââââââââ| 469/469 [00:01<00:00, 266.55it/s]\n",
      "Train Epoch: 66 [58880/60000 ( 98%)]  Loss: 0.013035: 100%|ââââââââââ| 469/469 [00:01<00:00, 281.66it/s]\n",
      "Train Epoch: 67 [58880/60000 ( 98%)]  Loss: 0.002875: 100%|ââââââââââ| 469/469 [00:01<00:00, 270.72it/s]\n",
      "Train Epoch: 68 [58880/60000 ( 98%)]  Loss: 0.002312: 100%|ââââââââââ| 469/469 [00:01<00:00, 265.75it/s]\n",
      "Train Epoch: 69 [58880/60000 ( 98%)]  Loss: 0.006107: 100%|ââââââââââ| 469/469 [00:01<00:00, 262.27it/s]\n",
      "Train Epoch: 70 [58880/60000 ( 98%)]  Loss: 0.001802: 100%|ââââââââââ| 469/469 [00:01<00:00, 242.72it/s]\n",
      "Train Epoch: 71 [58880/60000 ( 98%)]  Loss: 0.013444: 100%|ââââââââââ| 469/469 [00:01<00:00, 260.90it/s]\n",
      "Train Epoch: 72 [58880/60000 ( 98%)]  Loss: 0.005555: 100%|ââââââââââ| 469/469 [00:01<00:00, 244.20it/s]\n",
      "Train Epoch: 73 [58880/60000 ( 98%)]  Loss: 0.015658: 100%|ââââââââââ| 469/469 [00:01<00:00, 275.66it/s]\n",
      "Train Epoch: 74 [58880/60000 ( 98%)]  Loss: 0.043490: 100%|ââââââââââ| 469/469 [00:01<00:00, 250.06it/s]\n",
      "Train Epoch: 75 [58880/60000 ( 98%)]  Loss: 0.001013: 100%|ââââââââââ| 469/469 [00:01<00:00, 266.05it/s]\n",
      "Train Epoch: 76 [58880/60000 ( 98%)]  Loss: 0.009401: 100%|ââââââââââ| 469/469 [00:01<00:00, 273.88it/s]\n",
      "Train Epoch: 77 [58880/60000 ( 98%)]  Loss: 0.001107: 100%|ââââââââââ| 469/469 [00:01<00:00, 277.91it/s]\n",
      "Train Epoch: 78 [58880/60000 ( 98%)]  Loss: 0.007672: 100%|ââââââââââ| 469/469 [00:01<00:00, 275.47it/s]\n",
      "Train Epoch: 79 [58880/60000 ( 98%)]  Loss: 0.004367: 100%|ââââââââââ| 469/469 [00:01<00:00, 287.06it/s]\n",
      "Train Epoch: 80 [58880/60000 ( 98%)]  Loss: 0.010239: 100%|ââââââââââ| 469/469 [00:01<00:00, 275.90it/s]\n",
      "Train Epoch: 81 [58880/60000 ( 98%)]  Loss: 0.003572: 100%|ââââââââââ| 469/469 [00:02<00:00, 225.56it/s]\n",
      "Train Epoch: 82 [58880/60000 ( 98%)]  Loss: 0.000208: 100%|ââââââââââ| 469/469 [00:01<00:00, 261.34it/s]\n",
      "Train Epoch: 83 [58880/60000 ( 98%)]  Loss: 0.015208: 100%|ââââââââââ| 469/469 [00:01<00:00, 263.11it/s]\n",
      "Train Epoch: 84 [58880/60000 ( 98%)]  Loss: 0.032373: 100%|ââââââââââ| 469/469 [00:02<00:00, 233.27it/s]\n",
      "Train Epoch: 85 [58880/60000 ( 98%)]  Loss: 0.017045: 100%|ââââââââââ| 469/469 [00:01<00:00, 277.16it/s]\n",
      "Train Epoch: 86 [58880/60000 ( 98%)]  Loss: 0.005088: 100%|ââââââââââ| 469/469 [00:01<00:00, 271.72it/s]\n",
      "Train Epoch: 87 [58880/60000 ( 98%)]  Loss: 0.011583: 100%|ââââââââââ| 469/469 [00:01<00:00, 270.47it/s]\n",
      "Train Epoch: 88 [58880/60000 ( 98%)]  Loss: 0.004176: 100%|ââââââââââ| 469/469 [00:01<00:00, 261.13it/s]\n",
      "Train Epoch: 89 [58880/60000 ( 98%)]  Loss: 0.000120: 100%|ââââââââââ| 469/469 [00:01<00:00, 270.74it/s]\n",
      "Train Epoch: 90 [58880/60000 ( 98%)]  Loss: 0.030223: 100%|ââââââââââ| 469/469 [00:01<00:00, 277.62it/s]\n",
      "Train Epoch: 91 [58880/60000 ( 98%)]  Loss: 0.007130: 100%|ââââââââââ| 469/469 [00:01<00:00, 282.88it/s]\n",
      "Train Epoch: 92 [58880/60000 ( 98%)]  Loss: 0.000795: 100%|ââââââââââ| 469/469 [00:01<00:00, 281.89it/s]\n",
      "Train Epoch: 93 [58880/60000 ( 98%)]  Loss: 0.001130: 100%|ââââââââââ| 469/469 [00:01<00:00, 281.18it/s]\n",
      "Train Epoch: 94 [58880/60000 ( 98%)]  Loss: 0.021418: 100%|ââââââââââ| 469/469 [00:01<00:00, 249.47it/s]\n",
      "Train Epoch: 95 [58880/60000 ( 98%)]  Loss: 0.000228: 100%|ââââââââââ| 469/469 [00:01<00:00, 265.09it/s]\n",
      "Train Epoch: 96 [58880/60000 ( 98%)]  Loss: 0.002372: 100%|ââââââââââ| 469/469 [00:01<00:00, 266.05it/s]\n",
      "Train Epoch: 97 [58880/60000 ( 98%)]  Loss: 0.000696: 100%|ââââââââââ| 469/469 [00:01<00:00, 243.87it/s]\n",
      "Train Epoch: 98 [58880/60000 ( 98%)]  Loss: 0.000782: 100%|ââââââââââ| 469/469 [00:01<00:00, 265.60it/s]\n",
      "Train Epoch: 99 [58880/60000 ( 98%)]  Loss: 0.024958: 100%|ââââââââââ| 469/469 [00:01<00:00, 259.98it/s]\n"
     ]
    }
   ],
   "source": [
    "model = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mo hinh\n",
    "from time import time \n",
    "\n",
    "def test(model):\n",
    "    start_time = time()\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%). Total time = {time() - start_time}')\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0729, Accuracy: 9814/10000 (98.14%). Total time = 0.4852917194366455\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# luu log vao file\n",
    "def save_log(filename, content):\n",
    "    with open(filename, 'a') as f:\n",
    "        content += \"\\n\"\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_log(LOG_FILE, f\"initial_accuracy {accuracy}\")\n",
    "torch.save(model, f\"save_models/initial_model.ptmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tinh toan so luong non-zeros parameters\n",
    "\n",
    "# Print number of non-zeros weight in model \n",
    "\n",
    "def print_nonzeros(model):\n",
    "    nonzero = total = 0\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'mask' in name:\n",
    "            continue\n",
    "        tensor = p.data.cpu().numpy()\n",
    "        nz_count = np.count_nonzero(tensor)\n",
    "        total_params = np.prod(tensor.shape)\n",
    "        nonzero += nz_count\n",
    "        total += total_params\n",
    "        print(f'{name:20} | nonzeros = {nz_count:7} / {total_params:7} ({100 * nz_count / total_params:6.2f}%) | total_pruned = {total_params - nz_count :7} | shape = {tensor.shape}')\n",
    "    print(f'alive: {nonzero}, pruned : {total - nonzero}, total: {total}, Compression rate : {total/nonzero:10.2f}x  ({100 * (total-nonzero) / total:6.2f}% pruned)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight           | nonzeros =  235200 /  235200 (100.00%) | total_pruned =       0 | shape = (300, 784)\n",
      "fc1.bias             | nonzeros =     300 /     300 (100.00%) | total_pruned =       0 | shape = (300,)\n",
      "fc2.weight           | nonzeros =   30000 /   30000 (100.00%) | total_pruned =       0 | shape = (100, 300)\n",
      "fc2.bias             | nonzeros =     100 /     100 (100.00%) | total_pruned =       0 | shape = (100,)\n",
      "fc3.weight           | nonzeros =    1000 /    1000 (100.00%) | total_pruned =       0 | shape = (10, 100)\n",
      "fc3.bias             | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 266610, pruned : 0, total: 266610, Compression rate :       1.00x  (  0.00% pruned)\n"
     ]
    }
   ],
   "source": [
    "print_nonzeros(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning with threshold : 0.07572954148054123 for layer fc1\n",
      "Pruning with threshold : 0.11814986914396286 for layer fc2\n",
      "Pruning with threshold : 0.3765159547328949 for layer fc3\n"
     ]
    }
   ],
   "source": [
    "# tien hanh cat tia\n",
    "\n",
    "# Pruning\n",
    "model.prune_by_std(SENSITIVITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.1196, Accuracy: 6300/10000 (63.00%). Total time = 0.40337085723876953\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight           | nonzeros =   13238 /  235200 (  5.63%) | total_pruned =  221962 | shape = (300, 784)\n",
      "fc1.bias             | nonzeros =     300 /     300 (100.00%) | total_pruned =       0 | shape = (300,)\n",
      "fc2.weight           | nonzeros =    2133 /   30000 (  7.11%) | total_pruned =   27867 | shape = (100, 300)\n",
      "fc2.bias             | nonzeros =     100 /     100 (100.00%) | total_pruned =       0 | shape = (100,)\n",
      "fc3.weight           | nonzeros =      57 /    1000 (  5.70%) | total_pruned =     943 | shape = (10, 100)\n",
      "fc3.bias             | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 15838, pruned : 250772, total: 266610, Compression rate :      16.83x  ( 94.06% pruned)\n"
     ]
    }
   ],
   "source": [
    "save_log(LOG_FILE, f\"accuracy_after_pruning {accuracy}\")\n",
    "print_nonzeros(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [58880/60000 ( 98%)]  Loss: 0.056688: 100%|ââââââââââ| 469/469 [00:01<00:00, 264.56it/s]\n",
      "Train Epoch: 1 [58880/60000 ( 98%)]  Loss: 0.073047: 100%|ââââââââââ| 469/469 [00:01<00:00, 272.23it/s]\n",
      "Train Epoch: 2 [58880/60000 ( 98%)]  Loss: 0.015479: 100%|ââââââââââ| 469/469 [00:01<00:00, 276.58it/s]\n",
      "Train Epoch: 3 [58880/60000 ( 98%)]  Loss: 0.014431: 100%|ââââââââââ| 469/469 [00:01<00:00, 278.36it/s]\n",
      "Train Epoch: 4 [58880/60000 ( 98%)]  Loss: 0.029483: 100%|ââââââââââ| 469/469 [00:01<00:00, 281.12it/s]\n",
      "Train Epoch: 5 [58880/60000 ( 98%)]  Loss: 0.018427: 100%|ââââââââââ| 469/469 [00:01<00:00, 246.14it/s]\n",
      "Train Epoch: 6 [58880/60000 ( 98%)]  Loss: 0.016479: 100%|ââââââââââ| 469/469 [00:01<00:00, 285.63it/s]\n",
      "Train Epoch: 7 [58880/60000 ( 98%)]  Loss: 0.023556: 100%|ââââââââââ| 469/469 [00:01<00:00, 275.64it/s]\n",
      "Train Epoch: 8 [58880/60000 ( 98%)]  Loss: 0.007535: 100%|ââââââââââ| 469/469 [00:01<00:00, 271.64it/s]\n",
      "Train Epoch: 9 [58880/60000 ( 98%)]  Loss: 0.013756: 100%|ââââââââââ| 469/469 [00:01<00:00, 287.46it/s]\n",
      "Train Epoch: 10 [58880/60000 ( 98%)]  Loss: 0.001951: 100%|ââââââââââ| 469/469 [00:01<00:00, 243.28it/s]\n",
      "Train Epoch: 11 [58880/60000 ( 98%)]  Loss: 0.009753: 100%|ââââââââââ| 469/469 [00:01<00:00, 270.20it/s]\n",
      "Train Epoch: 12 [58880/60000 ( 98%)]  Loss: 0.010426: 100%|ââââââââââ| 469/469 [00:01<00:00, 261.81it/s]\n",
      "Train Epoch: 13 [58880/60000 ( 98%)]  Loss: 0.001712: 100%|ââââââââââ| 469/469 [00:01<00:00, 276.55it/s]\n",
      "Train Epoch: 14 [58880/60000 ( 98%)]  Loss: 0.003197: 100%|ââââââââââ| 469/469 [00:01<00:00, 266.50it/s]\n",
      "Train Epoch: 15 [58880/60000 ( 98%)]  Loss: 0.026693: 100%|ââââââââââ| 469/469 [00:01<00:00, 261.31it/s]\n",
      "Train Epoch: 16 [58880/60000 ( 98%)]  Loss: 0.001946: 100%|ââââââââââ| 469/469 [00:01<00:00, 277.11it/s]\n",
      "Train Epoch: 17 [58880/60000 ( 98%)]  Loss: 0.005399: 100%|ââââââââââ| 469/469 [00:01<00:00, 271.47it/s]\n",
      "Train Epoch: 18 [58880/60000 ( 98%)]  Loss: 0.002179: 100%|ââââââââââ| 469/469 [00:01<00:00, 275.91it/s]\n",
      "Train Epoch: 19 [58880/60000 ( 98%)]  Loss: 0.006509: 100%|ââââââââââ| 469/469 [00:01<00:00, 277.00it/s]\n",
      "Train Epoch: 20 [58880/60000 ( 98%)]  Loss: 0.003130: 100%|ââââââââââ| 469/469 [00:01<00:00, 274.77it/s]\n",
      "Train Epoch: 21 [58880/60000 ( 98%)]  Loss: 0.004648: 100%|ââââââââââ| 469/469 [00:01<00:00, 275.16it/s]\n",
      "Train Epoch: 22 [58880/60000 ( 98%)]  Loss: 0.009004: 100%|ââââââââââ| 469/469 [00:01<00:00, 265.08it/s]\n",
      "Train Epoch: 23 [58880/60000 ( 98%)]  Loss: 0.002347: 100%|ââââââââââ| 469/469 [00:01<00:00, 256.72it/s]\n",
      "Train Epoch: 24 [58880/60000 ( 98%)]  Loss: 0.006700: 100%|ââââââââââ| 469/469 [00:01<00:00, 242.35it/s]\n",
      "Train Epoch: 25 [58880/60000 ( 98%)]  Loss: 0.004706: 100%|ââââââââââ| 469/469 [00:01<00:00, 267.58it/s]\n",
      "Train Epoch: 26 [58880/60000 ( 98%)]  Loss: 0.004664: 100%|ââââââââââ| 469/469 [00:01<00:00, 271.37it/s]\n",
      "Train Epoch: 27 [58880/60000 ( 98%)]  Loss: 0.001692: 100%|ââââââââââ| 469/469 [00:01<00:00, 264.35it/s]\n",
      "Train Epoch: 28 [58880/60000 ( 98%)]  Loss: 0.003433: 100%|ââââââââââ| 469/469 [00:01<00:00, 257.26it/s]\n",
      "Train Epoch: 29 [58880/60000 ( 98%)]  Loss: 0.008502: 100%|ââââââââââ| 469/469 [00:01<00:00, 278.42it/s]\n",
      "Train Epoch: 30 [58880/60000 ( 98%)]  Loss: 0.013879: 100%|ââââââââââ| 469/469 [00:01<00:00, 266.49it/s]\n",
      "Train Epoch: 31 [58880/60000 ( 98%)]  Loss: 0.001288: 100%|ââââââââââ| 469/469 [00:01<00:00, 256.52it/s]\n",
      "Train Epoch: 32 [58880/60000 ( 98%)]  Loss: 0.004458: 100%|ââââââââââ| 469/469 [00:01<00:00, 249.75it/s]\n",
      "Train Epoch: 33 [58880/60000 ( 98%)]  Loss: 0.008869: 100%|ââââââââââ| 469/469 [00:01<00:00, 250.39it/s]\n",
      "Train Epoch: 34 [58880/60000 ( 98%)]  Loss: 0.007823: 100%|ââââââââââ| 469/469 [00:01<00:00, 258.95it/s]\n",
      "Train Epoch: 35 [58880/60000 ( 98%)]  Loss: 0.001664: 100%|ââââââââââ| 469/469 [00:01<00:00, 265.60it/s]\n",
      "Train Epoch: 36 [58880/60000 ( 98%)]  Loss: 0.002335: 100%|ââââââââââ| 469/469 [00:01<00:00, 244.78it/s]\n",
      "Train Epoch: 37 [58880/60000 ( 98%)]  Loss: 0.008507: 100%|ââââââââââ| 469/469 [00:01<00:00, 272.37it/s]\n",
      "Train Epoch: 38 [58880/60000 ( 98%)]  Loss: 0.002293: 100%|ââââââââââ| 469/469 [00:02<00:00, 222.61it/s]\n",
      "Train Epoch: 39 [58880/60000 ( 98%)]  Loss: 0.028478: 100%|ââââââââââ| 469/469 [00:01<00:00, 268.72it/s]\n",
      "Train Epoch: 40 [58880/60000 ( 98%)]  Loss: 0.000889: 100%|ââââââââââ| 469/469 [00:01<00:00, 252.11it/s]\n",
      "Train Epoch: 41 [58880/60000 ( 98%)]  Loss: 0.003875: 100%|ââââââââââ| 469/469 [00:01<00:00, 260.38it/s]\n",
      "Train Epoch: 42 [58880/60000 ( 98%)]  Loss: 0.015315: 100%|ââââââââââ| 469/469 [00:01<00:00, 255.92it/s]\n",
      "Train Epoch: 43 [58880/60000 ( 98%)]  Loss: 0.006481: 100%|ââââââââââ| 469/469 [00:01<00:00, 282.84it/s]\n",
      "Train Epoch: 44 [58880/60000 ( 98%)]  Loss: 0.008132: 100%|ââââââââââ| 469/469 [00:01<00:00, 268.90it/s]\n",
      "Train Epoch: 45 [58880/60000 ( 98%)]  Loss: 0.002129: 100%|ââââââââââ| 469/469 [00:01<00:00, 283.90it/s]\n",
      "Train Epoch: 46 [58880/60000 ( 98%)]  Loss: 0.004445: 100%|ââââââââââ| 469/469 [00:01<00:00, 257.32it/s]\n",
      "Train Epoch: 47 [58880/60000 ( 98%)]  Loss: 0.002147: 100%|ââââââââââ| 469/469 [00:01<00:00, 261.77it/s]\n",
      "Train Epoch: 48 [58880/60000 ( 98%)]  Loss: 0.007571: 100%|ââââââââââ| 469/469 [00:01<00:00, 282.82it/s]\n",
      "Train Epoch: 49 [58880/60000 ( 98%)]  Loss: 0.003052: 100%|ââââââââââ| 469/469 [00:01<00:00, 269.13it/s]\n",
      "Train Epoch: 50 [58880/60000 ( 98%)]  Loss: 0.003278: 100%|ââââââââââ| 469/469 [00:01<00:00, 270.91it/s]\n",
      "Train Epoch: 51 [58880/60000 ( 98%)]  Loss: 0.001847: 100%|ââââââââââ| 469/469 [00:01<00:00, 266.69it/s]\n",
      "Train Epoch: 52 [58880/60000 ( 98%)]  Loss: 0.007453: 100%|ââââââââââ| 469/469 [00:01<00:00, 266.48it/s]\n",
      "Train Epoch: 53 [58880/60000 ( 98%)]  Loss: 0.005026: 100%|ââââââââââ| 469/469 [00:01<00:00, 259.50it/s]\n",
      "Train Epoch: 54 [58880/60000 ( 98%)]  Loss: 0.003828: 100%|ââââââââââ| 469/469 [00:01<00:00, 267.28it/s]\n",
      "Train Epoch: 55 [58880/60000 ( 98%)]  Loss: 0.000553: 100%|ââââââââââ| 469/469 [00:01<00:00, 278.20it/s]\n",
      "Train Epoch: 56 [58880/60000 ( 98%)]  Loss: 0.000623: 100%|ââââââââââ| 469/469 [00:01<00:00, 279.25it/s]\n",
      "Train Epoch: 57 [58880/60000 ( 98%)]  Loss: 0.017954: 100%|ââââââââââ| 469/469 [00:01<00:00, 266.76it/s]\n",
      "Train Epoch: 58 [58880/60000 ( 98%)]  Loss: 0.002841: 100%|ââââââââââ| 469/469 [00:01<00:00, 243.82it/s]\n",
      "Train Epoch: 59 [58880/60000 ( 98%)]  Loss: 0.000688: 100%|ââââââââââ| 469/469 [00:01<00:00, 271.48it/s]\n",
      "Train Epoch: 60 [58880/60000 ( 98%)]  Loss: 0.001131: 100%|ââââââââââ| 469/469 [00:01<00:00, 252.36it/s]\n",
      "Train Epoch: 61 [58880/60000 ( 98%)]  Loss: 0.005559: 100%|ââââââââââ| 469/469 [00:01<00:00, 277.57it/s]\n",
      "Train Epoch: 62 [58880/60000 ( 98%)]  Loss: 0.002612: 100%|ââââââââââ| 469/469 [00:01<00:00, 275.63it/s]\n",
      "Train Epoch: 63 [58880/60000 ( 98%)]  Loss: 0.005035: 100%|ââââââââââ| 469/469 [00:01<00:00, 266.89it/s]\n",
      "Train Epoch: 64 [58880/60000 ( 98%)]  Loss: 0.000972: 100%|ââââââââââ| 469/469 [00:01<00:00, 290.15it/s]\n",
      "Train Epoch: 65 [58880/60000 ( 98%)]  Loss: 0.002209: 100%|ââââââââââ| 469/469 [00:01<00:00, 265.61it/s]\n",
      "Train Epoch: 66 [58880/60000 ( 98%)]  Loss: 0.003067: 100%|ââââââââââ| 469/469 [00:01<00:00, 250.36it/s]\n",
      "Train Epoch: 67 [58880/60000 ( 98%)]  Loss: 0.000911: 100%|ââââââââââ| 469/469 [00:01<00:00, 264.57it/s]\n",
      "Train Epoch: 68 [58880/60000 ( 98%)]  Loss: 0.000877: 100%|ââââââââââ| 469/469 [00:01<00:00, 261.02it/s]\n",
      "Train Epoch: 69 [58880/60000 ( 98%)]  Loss: 0.015017: 100%|ââââââââââ| 469/469 [00:01<00:00, 275.59it/s]\n",
      "Train Epoch: 70 [58880/60000 ( 98%)]  Loss: 0.006736: 100%|ââââââââââ| 469/469 [00:01<00:00, 267.39it/s]\n",
      "Train Epoch: 71 [58880/60000 ( 98%)]  Loss: 0.002926: 100%|ââââââââââ| 469/469 [00:01<00:00, 269.63it/s]\n",
      "Train Epoch: 72 [58880/60000 ( 98%)]  Loss: 0.028413: 100%|ââââââââââ| 469/469 [00:01<00:00, 269.00it/s]\n",
      "Train Epoch: 73 [58880/60000 ( 98%)]  Loss: 0.023394: 100%|ââââââââââ| 469/469 [00:01<00:00, 265.89it/s]\n",
      "Train Epoch: 74 [58880/60000 ( 98%)]  Loss: 0.001137: 100%|ââââââââââ| 469/469 [00:01<00:00, 279.77it/s]\n",
      "Train Epoch: 75 [58880/60000 ( 98%)]  Loss: 0.001028: 100%|ââââââââââ| 469/469 [00:01<00:00, 262.72it/s]\n",
      "Train Epoch: 76 [58880/60000 ( 98%)]  Loss: 0.001590: 100%|ââââââââââ| 469/469 [00:01<00:00, 265.95it/s]\n",
      "Train Epoch: 77 [58880/60000 ( 98%)]  Loss: 0.005839: 100%|ââââââââââ| 469/469 [00:01<00:00, 266.10it/s]\n",
      "Train Epoch: 78 [58880/60000 ( 98%)]  Loss: 0.001731: 100%|ââââââââââ| 469/469 [00:01<00:00, 256.06it/s]\n",
      "Train Epoch: 79 [58880/60000 ( 98%)]  Loss: 0.000344: 100%|ââââââââââ| 469/469 [00:02<00:00, 220.85it/s]\n",
      "Train Epoch: 80 [58880/60000 ( 98%)]  Loss: 0.002702: 100%|ââââââââââ| 469/469 [00:01<00:00, 258.16it/s]\n",
      "Train Epoch: 81 [58880/60000 ( 98%)]  Loss: 0.007942: 100%|ââââââââââ| 469/469 [00:01<00:00, 283.24it/s]\n",
      "Train Epoch: 82 [58880/60000 ( 98%)]  Loss: 0.003186: 100%|ââââââââââ| 469/469 [00:01<00:00, 272.78it/s]\n",
      "Train Epoch: 83 [58880/60000 ( 98%)]  Loss: 0.004623: 100%|ââââââââââ| 469/469 [00:01<00:00, 286.48it/s]\n",
      "Train Epoch: 84 [58880/60000 ( 98%)]  Loss: 0.000709: 100%|ââââââââââ| 469/469 [00:01<00:00, 281.32it/s]\n",
      "Train Epoch: 85 [58880/60000 ( 98%)]  Loss: 0.001996: 100%|ââââââââââ| 469/469 [00:01<00:00, 286.55it/s]\n",
      "Train Epoch: 86 [58880/60000 ( 98%)]  Loss: 0.007595: 100%|ââââââââââ| 469/469 [00:01<00:00, 283.67it/s]\n",
      "Train Epoch: 87 [58880/60000 ( 98%)]  Loss: 0.001377: 100%|ââââââââââ| 469/469 [00:01<00:00, 276.88it/s]\n",
      "Train Epoch: 88 [58880/60000 ( 98%)]  Loss: 0.000816: 100%|ââââââââââ| 469/469 [00:01<00:00, 282.64it/s]\n",
      "Train Epoch: 89 [58880/60000 ( 98%)]  Loss: 0.001879: 100%|ââââââââââ| 469/469 [00:01<00:00, 281.25it/s]\n",
      "Train Epoch: 90 [58880/60000 ( 98%)]  Loss: 0.005114: 100%|ââââââââââ| 469/469 [00:01<00:00, 281.83it/s]\n",
      "Train Epoch: 91 [58880/60000 ( 98%)]  Loss: 0.001652: 100%|ââââââââââ| 469/469 [00:01<00:00, 279.36it/s]\n",
      "Train Epoch: 92 [58880/60000 ( 98%)]  Loss: 0.012716: 100%|ââââââââââ| 469/469 [00:01<00:00, 279.56it/s]\n",
      "Train Epoch: 93 [58880/60000 ( 98%)]  Loss: 0.002304: 100%|ââââââââââ| 469/469 [00:01<00:00, 291.19it/s]\n",
      "Train Epoch: 94 [58880/60000 ( 98%)]  Loss: 0.005689: 100%|ââââââââââ| 469/469 [00:01<00:00, 275.63it/s]\n",
      "Train Epoch: 95 [58880/60000 ( 98%)]  Loss: 0.000944: 100%|ââââââââââ| 469/469 [00:01<00:00, 278.20it/s]\n",
      "Train Epoch: 96 [58880/60000 ( 98%)]  Loss: 0.002768: 100%|ââââââââââ| 469/469 [00:01<00:00, 265.39it/s]\n",
      "Train Epoch: 97 [58880/60000 ( 98%)]  Loss: 0.011580: 100%|ââââââââââ| 469/469 [00:02<00:00, 217.58it/s]\n",
      "Train Epoch: 98 [58880/60000 ( 98%)]  Loss: 0.005348: 100%|ââââââââââ| 469/469 [00:02<00:00, 224.04it/s]\n",
      "Train Epoch: 99 [58880/60000 ( 98%)]  Loss: 0.001998: 100%|ââââââââââ| 469/469 [00:01<00:00, 274.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Retraining\n",
    "optimizer.load_state_dict(initial_optimizer_state_dict) # Reset the optimizer\n",
    "\n",
    "model = train(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0729, Accuracy: 9820/10000 (98.20%). Total time = 0.3879213333129883\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_log(LOG_FILE, f\"accuracy_after_retraining {accuracy}\")\n",
    "torch.save(model, f\"save_models/model_after_retraining.ptmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LÆ°á»£ng tá»­ hÃ³a vÃ  Share weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import csc_matrix, csr_matrix #hai format de luu tru ma tran thu nham tinh toan duoc de dang do tiet kiem bo nho.\n",
    "\n",
    "def apply_weight_sharing(model, bits=5): # co toi da 2^5 = 32 cum cua kmeans\n",
    "    for module in model.children():\n",
    "        dev = module.weight.device\n",
    "        weight = module.weight.data.cpu().numpy()\n",
    "        shape = weight.shape\n",
    "        mat = csr_matrix(weight) if shape[0] < shape[1] else csc_matrix(weight)\n",
    "        min_ = min(mat.data)\n",
    "        max_ = max(mat.data)\n",
    "        space = np.linspace(min_, max_, num=2**bits)\n",
    "        kmeans = KMeans(n_clusters=len(space), init=space.reshape(-1,1), n_init=1, algorithm=\"full\")\n",
    "        kmeans.fit(mat.data.reshape(-1,1))\n",
    "        new_weight = kmeans.cluster_centers_[kmeans.labels_].reshape(-1) # share láº¡i centroid vÃ o cÃ¡c vá» trÃ­ weight\n",
    "        mat.data = new_weight\n",
    "        module.weight.data = torch.from_numpy(mat.toarray()).to(dev)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maicg/anaconda3/envs/Face-insight/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1318: FutureWarning: algorithm='full' is deprecated, it will be removed in 1.3. Using 'lloyd' instead.\n",
      "  warnings.warn(\n",
      "/home/maicg/anaconda3/envs/Face-insight/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1318: FutureWarning: algorithm='full' is deprecated, it will be removed in 1.3. Using 'lloyd' instead.\n",
      "  warnings.warn(\n",
      "/home/maicg/anaconda3/envs/Face-insight/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1318: FutureWarning: algorithm='full' is deprecated, it will be removed in 1.3. Using 'lloyd' instead.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_684699/1534380546.py:14: ConvergenceWarning: Number of distinct clusters (30) found smaller than n_clusters (32). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(mat.data.reshape(-1,1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (fc1): MaskedLinear(in_features=784, out_features=300, bias=True)\n",
       "  (fc2): MaskedLinear(in_features=300, out_features=100, bias=True)\n",
       "  (fc3): MaskedLinear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_weight_sharing(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0737, Accuracy: 9823/10000 (98.23%). Total time = 0.4220414161682129\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Face-insight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
