{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maicg/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square conv kernel\n",
    "        # self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(784, 300)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cai dat mot so hyperparemeter\n",
    "# Define some const\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "USE_CUDA = True\n",
    "SEED = 42\n",
    "LOG_AFTER = 10 # How many batches to wait before logging training status\n",
    "LOG_FILE = 'log_prunting.txt'\n",
    "SENSITIVITY = 2 # Sensitivity value that is multiplied to layer's std in order to get threshold value\n",
    "\n",
    "# Control Seed\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Select Device\n",
    "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cai dat dataloader\n",
    "# Create the dataset with MNIST\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Train loader\n",
    "kwargs = {'num_workers': 5, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "# Test loader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dinh nghia optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define optimizer with Adam function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n",
    "initial_optimizer_state_dict = optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training mo hinh\n",
    "\n",
    "from tqdm import tqdm \n",
    "# Define training function \n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        for batch_idx, (data, target) in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # zero-out all the gradients corresponding to the pruned connections\n",
    "            for name, p in model.named_parameters():\n",
    "                tensor = p.data.cpu().numpy()\n",
    "                grad_tensor = p.grad.data.cpu().numpy()\n",
    "                grad_tensor = np.where(tensor==0, 0, grad_tensor)\n",
    "                p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "\n",
    "            optimizer.step()\n",
    "            if batch_idx % LOG_AFTER == 0:\n",
    "                done = batch_idx * len(data)\n",
    "                percentage = 100. * batch_idx / len(train_loader)\n",
    "                pbar.set_description(f'Train Epoch: {epoch} [{done:5}/{len(train_loader.dataset)} ({percentage:3.0f}%)]  Loss: {loss.item():.6f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [58880/60000 ( 98%)]  Loss: 0.165757: 100%|██████████| 469/469 [00:01<00:00, 271.60it/s]\n",
      "Train Epoch: 1 [58880/60000 ( 98%)]  Loss: 0.077967: 100%|██████████| 469/469 [00:01<00:00, 296.10it/s]\n",
      "Train Epoch: 2 [58880/60000 ( 98%)]  Loss: 0.239732: 100%|██████████| 469/469 [00:01<00:00, 296.09it/s]\n",
      "Train Epoch: 3 [58880/60000 ( 98%)]  Loss: 0.054293: 100%|██████████| 469/469 [00:01<00:00, 285.10it/s]\n",
      "Train Epoch: 4 [58880/60000 ( 98%)]  Loss: 0.013612: 100%|██████████| 469/469 [00:01<00:00, 292.28it/s]\n",
      "Train Epoch: 5 [58880/60000 ( 98%)]  Loss: 0.056440: 100%|██████████| 469/469 [00:01<00:00, 252.28it/s]\n",
      "Train Epoch: 6 [58880/60000 ( 98%)]  Loss: 0.052944: 100%|██████████| 469/469 [00:01<00:00, 289.32it/s]\n",
      "Train Epoch: 7 [58880/60000 ( 98%)]  Loss: 0.012814: 100%|██████████| 469/469 [00:01<00:00, 294.21it/s]\n",
      "Train Epoch: 8 [58880/60000 ( 98%)]  Loss: 0.008846: 100%|██████████| 469/469 [00:01<00:00, 275.62it/s]\n",
      "Train Epoch: 9 [58880/60000 ( 98%)]  Loss: 0.018314: 100%|██████████| 469/469 [00:01<00:00, 291.22it/s]\n",
      "Train Epoch: 10 [58880/60000 ( 98%)]  Loss: 0.056468: 100%|██████████| 469/469 [00:01<00:00, 270.00it/s]\n",
      "Train Epoch: 11 [58880/60000 ( 98%)]  Loss: 0.008436: 100%|██████████| 469/469 [00:01<00:00, 295.98it/s]\n",
      "Train Epoch: 12 [58880/60000 ( 98%)]  Loss: 0.015874: 100%|██████████| 469/469 [00:01<00:00, 271.51it/s]\n",
      "Train Epoch: 13 [58880/60000 ( 98%)]  Loss: 0.022259: 100%|██████████| 469/469 [00:01<00:00, 272.85it/s]\n",
      "Train Epoch: 14 [58880/60000 ( 98%)]  Loss: 0.023746: 100%|██████████| 469/469 [00:01<00:00, 294.83it/s]\n",
      "Train Epoch: 15 [58880/60000 ( 98%)]  Loss: 0.027733: 100%|██████████| 469/469 [00:01<00:00, 280.31it/s]\n",
      "Train Epoch: 16 [58880/60000 ( 98%)]  Loss: 0.009845: 100%|██████████| 469/469 [00:01<00:00, 296.93it/s]\n",
      "Train Epoch: 17 [58880/60000 ( 98%)]  Loss: 0.001611: 100%|██████████| 469/469 [00:01<00:00, 277.97it/s]\n",
      "Train Epoch: 18 [58880/60000 ( 98%)]  Loss: 0.025716: 100%|██████████| 469/469 [00:01<00:00, 289.37it/s]\n",
      "Train Epoch: 19 [58880/60000 ( 98%)]  Loss: 0.024072: 100%|██████████| 469/469 [00:01<00:00, 325.84it/s]\n",
      "Train Epoch: 20 [58880/60000 ( 98%)]  Loss: 0.006763: 100%|██████████| 469/469 [00:01<00:00, 285.42it/s]\n",
      "Train Epoch: 21 [58880/60000 ( 98%)]  Loss: 0.018813: 100%|██████████| 469/469 [00:01<00:00, 297.20it/s]\n",
      "Train Epoch: 22 [58880/60000 ( 98%)]  Loss: 0.009873: 100%|██████████| 469/469 [00:01<00:00, 298.94it/s]\n",
      "Train Epoch: 23 [58880/60000 ( 98%)]  Loss: 0.009468: 100%|██████████| 469/469 [00:01<00:00, 316.20it/s]\n",
      "Train Epoch: 24 [58880/60000 ( 98%)]  Loss: 0.060188: 100%|██████████| 469/469 [00:01<00:00, 287.94it/s]\n",
      "Train Epoch: 25 [58880/60000 ( 98%)]  Loss: 0.007508: 100%|██████████| 469/469 [00:01<00:00, 309.04it/s]\n",
      "Train Epoch: 26 [58880/60000 ( 98%)]  Loss: 0.050703: 100%|██████████| 469/469 [00:01<00:00, 287.84it/s]\n",
      "Train Epoch: 27 [58880/60000 ( 98%)]  Loss: 0.011845: 100%|██████████| 469/469 [00:01<00:00, 314.80it/s]\n",
      "Train Epoch: 28 [58880/60000 ( 98%)]  Loss: 0.011789: 100%|██████████| 469/469 [00:01<00:00, 313.35it/s]\n",
      "Train Epoch: 29 [58880/60000 ( 98%)]  Loss: 0.004736: 100%|██████████| 469/469 [00:01<00:00, 324.33it/s]\n",
      "Train Epoch: 30 [58880/60000 ( 98%)]  Loss: 0.002794: 100%|██████████| 469/469 [00:01<00:00, 300.21it/s]\n",
      "Train Epoch: 31 [58880/60000 ( 98%)]  Loss: 0.006540: 100%|██████████| 469/469 [00:01<00:00, 315.35it/s]\n",
      "Train Epoch: 32 [58880/60000 ( 98%)]  Loss: 0.002058: 100%|██████████| 469/469 [00:01<00:00, 303.10it/s]\n",
      "Train Epoch: 33 [58880/60000 ( 98%)]  Loss: 0.020814: 100%|██████████| 469/469 [00:01<00:00, 323.70it/s]\n",
      "Train Epoch: 34 [58880/60000 ( 98%)]  Loss: 0.003335: 100%|██████████| 469/469 [00:01<00:00, 314.61it/s]\n",
      "Train Epoch: 35 [58880/60000 ( 98%)]  Loss: 0.000994: 100%|██████████| 469/469 [00:01<00:00, 330.54it/s]\n",
      "Train Epoch: 36 [58880/60000 ( 98%)]  Loss: 0.001193: 100%|██████████| 469/469 [00:01<00:00, 314.92it/s]\n",
      "Train Epoch: 37 [58880/60000 ( 98%)]  Loss: 0.017049: 100%|██████████| 469/469 [00:01<00:00, 314.30it/s]\n",
      "Train Epoch: 38 [58880/60000 ( 98%)]  Loss: 0.039115: 100%|██████████| 469/469 [00:01<00:00, 328.94it/s]\n",
      "Train Epoch: 39 [58880/60000 ( 98%)]  Loss: 0.010864: 100%|██████████| 469/469 [00:01<00:00, 315.85it/s]\n",
      "Train Epoch: 40 [58880/60000 ( 98%)]  Loss: 0.015194: 100%|██████████| 469/469 [00:01<00:00, 329.97it/s]\n",
      "Train Epoch: 41 [58880/60000 ( 98%)]  Loss: 0.013936: 100%|██████████| 469/469 [00:01<00:00, 238.58it/s]\n",
      "Train Epoch: 42 [58880/60000 ( 98%)]  Loss: 0.013965: 100%|██████████| 469/469 [00:01<00:00, 279.42it/s]\n",
      "Train Epoch: 43 [58880/60000 ( 98%)]  Loss: 0.016029: 100%|██████████| 469/469 [00:01<00:00, 276.91it/s]\n",
      "Train Epoch: 44 [58880/60000 ( 98%)]  Loss: 0.014675: 100%|██████████| 469/469 [00:02<00:00, 233.00it/s]\n",
      "Train Epoch: 45 [58880/60000 ( 98%)]  Loss: 0.030383: 100%|██████████| 469/469 [00:01<00:00, 262.27it/s]\n",
      "Train Epoch: 46 [58880/60000 ( 98%)]  Loss: 0.011998: 100%|██████████| 469/469 [00:01<00:00, 242.18it/s]\n",
      "Train Epoch: 47 [58880/60000 ( 98%)]  Loss: 0.011493: 100%|██████████| 469/469 [00:01<00:00, 263.50it/s]\n",
      "Train Epoch: 48 [58880/60000 ( 98%)]  Loss: 0.000718: 100%|██████████| 469/469 [00:01<00:00, 256.04it/s]\n",
      "Train Epoch: 49 [58880/60000 ( 98%)]  Loss: 0.001460: 100%|██████████| 469/469 [00:01<00:00, 235.74it/s]\n",
      "Train Epoch: 50 [58880/60000 ( 98%)]  Loss: 0.005319: 100%|██████████| 469/469 [00:02<00:00, 230.82it/s]\n",
      "Train Epoch: 51 [58880/60000 ( 98%)]  Loss: 0.040121: 100%|██████████| 469/469 [00:01<00:00, 282.27it/s]\n",
      "Train Epoch: 52 [58880/60000 ( 98%)]  Loss: 0.011874: 100%|██████████| 469/469 [00:01<00:00, 332.34it/s]\n",
      "Train Epoch: 53 [58880/60000 ( 98%)]  Loss: 0.011795: 100%|██████████| 469/469 [00:01<00:00, 256.53it/s]\n",
      "Train Epoch: 54 [58880/60000 ( 98%)]  Loss: 0.002204: 100%|██████████| 469/469 [00:01<00:00, 252.29it/s]\n",
      "Train Epoch: 55 [58880/60000 ( 98%)]  Loss: 0.014832: 100%|██████████| 469/469 [00:01<00:00, 238.69it/s]\n",
      "Train Epoch: 56 [58880/60000 ( 98%)]  Loss: 0.001292: 100%|██████████| 469/469 [00:02<00:00, 220.98it/s]\n",
      "Train Epoch: 57 [58880/60000 ( 98%)]  Loss: 0.004473: 100%|██████████| 469/469 [00:01<00:00, 287.50it/s]\n",
      "Train Epoch: 58 [58880/60000 ( 98%)]  Loss: 0.014271: 100%|██████████| 469/469 [00:01<00:00, 299.73it/s]\n",
      "Train Epoch: 59 [58880/60000 ( 98%)]  Loss: 0.002666: 100%|██████████| 469/469 [00:01<00:00, 301.74it/s]\n",
      "Train Epoch: 60 [58880/60000 ( 98%)]  Loss: 0.001908: 100%|██████████| 469/469 [00:01<00:00, 273.70it/s]\n",
      "Train Epoch: 61 [58880/60000 ( 98%)]  Loss: 0.007944: 100%|██████████| 469/469 [00:01<00:00, 308.63it/s]\n",
      "Train Epoch: 62 [58880/60000 ( 98%)]  Loss: 0.002214: 100%|██████████| 469/469 [00:02<00:00, 213.57it/s]\n",
      "Train Epoch: 63 [58880/60000 ( 98%)]  Loss: 0.000566: 100%|██████████| 469/469 [00:01<00:00, 255.09it/s]\n",
      "Train Epoch: 64 [58880/60000 ( 98%)]  Loss: 0.020345: 100%|██████████| 469/469 [00:02<00:00, 233.90it/s]\n",
      "Train Epoch: 65 [58880/60000 ( 98%)]  Loss: 0.007167: 100%|██████████| 469/469 [00:01<00:00, 313.40it/s]\n",
      "Train Epoch: 66 [58880/60000 ( 98%)]  Loss: 0.009151: 100%|██████████| 469/469 [00:01<00:00, 297.93it/s]\n",
      "Train Epoch: 67 [58880/60000 ( 98%)]  Loss: 0.019674: 100%|██████████| 469/469 [00:01<00:00, 243.32it/s]\n",
      "Train Epoch: 68 [58880/60000 ( 98%)]  Loss: 0.012457: 100%|██████████| 469/469 [00:01<00:00, 290.64it/s]\n",
      "Train Epoch: 69 [58880/60000 ( 98%)]  Loss: 0.002157: 100%|██████████| 469/469 [00:01<00:00, 258.18it/s]\n",
      "Train Epoch: 70 [58880/60000 ( 98%)]  Loss: 0.033834: 100%|██████████| 469/469 [00:01<00:00, 261.79it/s]\n",
      "Train Epoch: 71 [58880/60000 ( 98%)]  Loss: 0.001419: 100%|██████████| 469/469 [00:01<00:00, 251.27it/s]\n",
      "Train Epoch: 72 [58880/60000 ( 98%)]  Loss: 0.008636: 100%|██████████| 469/469 [00:01<00:00, 337.21it/s]\n",
      "Train Epoch: 73 [58880/60000 ( 98%)]  Loss: 0.000844: 100%|██████████| 469/469 [00:01<00:00, 306.95it/s]\n",
      "Train Epoch: 74 [58880/60000 ( 98%)]  Loss: 0.050961: 100%|██████████| 469/469 [00:01<00:00, 328.06it/s]\n",
      "Train Epoch: 75 [58880/60000 ( 98%)]  Loss: 0.011081: 100%|██████████| 469/469 [00:01<00:00, 312.13it/s]\n",
      "Train Epoch: 76 [58880/60000 ( 98%)]  Loss: 0.005149: 100%|██████████| 469/469 [00:01<00:00, 336.87it/s]\n",
      "Train Epoch: 77 [58880/60000 ( 98%)]  Loss: 0.001537: 100%|██████████| 469/469 [00:01<00:00, 315.25it/s]\n",
      "Train Epoch: 78 [58880/60000 ( 98%)]  Loss: 0.027967: 100%|██████████| 469/469 [00:01<00:00, 331.13it/s]\n",
      "Train Epoch: 79 [58880/60000 ( 98%)]  Loss: 0.000371: 100%|██████████| 469/469 [00:01<00:00, 315.86it/s]\n",
      "Train Epoch: 80 [58880/60000 ( 98%)]  Loss: 0.012602: 100%|██████████| 469/469 [00:01<00:00, 329.98it/s]\n",
      "Train Epoch: 81 [58880/60000 ( 98%)]  Loss: 0.002092: 100%|██████████| 469/469 [00:01<00:00, 327.36it/s]\n",
      "Train Epoch: 82 [58880/60000 ( 98%)]  Loss: 0.002486: 100%|██████████| 469/469 [00:01<00:00, 314.75it/s]\n",
      "Train Epoch: 83 [58880/60000 ( 98%)]  Loss: 0.001032: 100%|██████████| 469/469 [00:01<00:00, 318.29it/s]\n",
      "Train Epoch: 84 [58880/60000 ( 98%)]  Loss: 0.000331: 100%|██████████| 469/469 [00:01<00:00, 313.68it/s]\n",
      "Train Epoch: 85 [58880/60000 ( 98%)]  Loss: 0.036685: 100%|██████████| 469/469 [00:01<00:00, 327.48it/s]\n",
      "Train Epoch: 86 [58880/60000 ( 98%)]  Loss: 0.002361: 100%|██████████| 469/469 [00:01<00:00, 308.64it/s]\n",
      "Train Epoch: 87 [58880/60000 ( 98%)]  Loss: 0.006133: 100%|██████████| 469/469 [00:01<00:00, 300.66it/s]\n",
      "Train Epoch: 88 [58880/60000 ( 98%)]  Loss: 0.000555: 100%|██████████| 469/469 [00:01<00:00, 300.11it/s]\n",
      "Train Epoch: 89 [58880/60000 ( 98%)]  Loss: 0.008660: 100%|██████████| 469/469 [00:01<00:00, 309.72it/s]\n",
      "Train Epoch: 90 [58880/60000 ( 98%)]  Loss: 0.001163: 100%|██████████| 469/469 [00:01<00:00, 332.60it/s]\n",
      "Train Epoch: 91 [58880/60000 ( 98%)]  Loss: 0.002794: 100%|██████████| 469/469 [00:01<00:00, 317.68it/s]\n",
      "Train Epoch: 92 [58880/60000 ( 98%)]  Loss: 0.000405: 100%|██████████| 469/469 [00:01<00:00, 335.30it/s]\n",
      "Train Epoch: 93 [58880/60000 ( 98%)]  Loss: 0.039459: 100%|██████████| 469/469 [00:01<00:00, 311.16it/s]\n",
      "Train Epoch: 94 [58880/60000 ( 98%)]  Loss: 0.000421: 100%|██████████| 469/469 [00:01<00:00, 335.01it/s]\n",
      "Train Epoch: 95 [58880/60000 ( 98%)]  Loss: 0.007502: 100%|██████████| 469/469 [00:01<00:00, 319.13it/s]\n",
      "Train Epoch: 96 [58880/60000 ( 98%)]  Loss: 0.001150: 100%|██████████| 469/469 [00:01<00:00, 316.35it/s]\n",
      "Train Epoch: 97 [58880/60000 ( 98%)]  Loss: 0.003476: 100%|██████████| 469/469 [00:01<00:00, 325.65it/s]\n",
      "Train Epoch: 98 [58880/60000 ( 98%)]  Loss: 0.002615: 100%|██████████| 469/469 [00:01<00:00, 318.78it/s]\n",
      "Train Epoch: 99 [58880/60000 ( 98%)]  Loss: 0.003301: 100%|██████████| 469/469 [00:01<00:00, 280.39it/s]\n"
     ]
    }
   ],
   "source": [
    "model = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mo hinh\n",
    "from time import time \n",
    "\n",
    "def test(model):\n",
    "    start_time = time()\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%). Total time = {time() - start_time}')\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0890, Accuracy: 9801/10000 (98.01%). Total time = 0.3305783271789551\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nonzeros(model):\n",
    "    nonzero = total = 0\n",
    "    for name, p in model.named_parameters():\n",
    "        tensor = p.data.cpu().numpy()\n",
    "        nz_count = np.count_nonzero(tensor)\n",
    "        total_params = np.prod(tensor.shape)\n",
    "        nonzero += nz_count\n",
    "        total += total_params\n",
    "        print(f'{name:20} | nonzeros = {nz_count:7} / {total_params:7} ({100 * nz_count / total_params:6.2f}%) | total_pruned = {total_params - nz_count :7} | shape = {tensor.shape}')\n",
    "    print(f'alive: {nonzero}, pruned : {total - nonzero}, total: {total}, Compression rate : {total/nonzero:10.2f}x  ({100 * (total-nonzero) / total:6.2f}% pruned)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight           | nonzeros =  235200 /  235200 (100.00%) | total_pruned =       0 | shape = (300, 784)\n",
      "fc1.bias             | nonzeros =     300 /     300 (100.00%) | total_pruned =       0 | shape = (300,)\n",
      "fc2.weight           | nonzeros =   30000 /   30000 (100.00%) | total_pruned =       0 | shape = (100, 300)\n",
      "fc2.bias             | nonzeros =     100 /     100 (100.00%) | total_pruned =       0 | shape = (100,)\n",
      "fc3.weight           | nonzeros =    1000 /    1000 (100.00%) | total_pruned =       0 | shape = (10, 100)\n",
      "fc3.bias             | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 266610, pruned : 0, total: 266610, Compression rate :       1.00x  (  0.00% pruned)\n"
     ]
    }
   ],
   "source": [
    "print_nonzeros(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Customize of prune function with mask\n",
    "def prune(module, threshold): #tinh toan lai cac trong so co weight nho hon nguong quy dinh, cap nhat lai mask va weight tai cac vi tri do ve gia tri 0\n",
    "    weight_dev = module.weight.device\n",
    "    mask_module =  Parameter(torch.ones([module.out_features, module.in_features]), requires_grad=False)\n",
    "    mask_dev = mask_module.device \n",
    "    # Convert Tensors to numpy and calculate\n",
    "    tensor = module.weight.data.cpu().numpy()\n",
    "    mask = mask_module.data.cpu().numpy()\n",
    "    new_mask = np.where(abs(tensor) < threshold, 0, mask)\n",
    "    # Apply new weight and mask\n",
    "    module.weight.data = torch.from_numpy(tensor * new_mask).to(weight_dev)\n",
    "    mask_module = torch.from_numpy(new_mask).to(mask_dev)\n",
    "\n",
    "def prune_by_std(module, s=0.25): # tuy chinh tham so s=0.25 de tinh toan gia tri cua threshold can cat tia, 25% của average standard deviation: gia tri do lech chuan trung binh\n",
    "    # Note that module here is the layer\n",
    "    # ex) fc1, fc2, fc3\n",
    "    threshold = np.std(module.weight.data.cpu().numpy()) * s\n",
    "    print(f'Pruning with threshold : {threshold} for layer')\n",
    "    prune(module, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning with threshold : 0.009447183459997177 for layer\n",
      "Pruning with threshold : 0.014730214141309261 for layer\n",
      "Pruning with threshold : 0.04688509926199913 for layer\n"
     ]
    }
   ],
   "source": [
    "prune_by_std(model.fc1)\n",
    "prune_by_std(model.fc2)\n",
    "prune_by_std(model.fc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0971, Accuracy: 9783/10000 (97.83%). Total time = 0.31531476974487305\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight           | nonzeros =   97114 /  235200 ( 41.29%) | total_pruned =  138086 | shape = (300, 784)\n",
      "fc1.bias             | nonzeros =     300 /     300 (100.00%) | total_pruned =       0 | shape = (300,)\n",
      "fc2.weight           | nonzeros =   13315 /   30000 ( 44.38%) | total_pruned =   16685 | shape = (100, 300)\n",
      "fc2.bias             | nonzeros =     100 /     100 (100.00%) | total_pruned =       0 | shape = (100,)\n",
      "fc3.weight           | nonzeros =     723 /    1000 ( 72.30%) | total_pruned =     277 | shape = (10, 100)\n",
      "fc3.bias             | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 111562, pruned : 155048, total: 266610, Compression rate :       2.39x  ( 58.16% pruned)\n"
     ]
    }
   ],
   "source": [
    "print_nonzeros(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [58880/60000 ( 98%)]  Loss: 0.000863: 100%|██████████| 469/469 [00:01<00:00, 263.28it/s]\n",
      "Train Epoch: 1 [58880/60000 ( 98%)]  Loss: 0.002972: 100%|██████████| 469/469 [00:01<00:00, 279.88it/s]\n",
      "Train Epoch: 2 [58880/60000 ( 98%)]  Loss: 0.008068: 100%|██████████| 469/469 [00:01<00:00, 254.22it/s]\n",
      "Train Epoch: 3 [58880/60000 ( 98%)]  Loss: 0.003958: 100%|██████████| 469/469 [00:02<00:00, 219.82it/s]\n",
      "Train Epoch: 4 [58880/60000 ( 98%)]  Loss: 0.000260: 100%|██████████| 469/469 [00:01<00:00, 288.83it/s]\n",
      "Train Epoch: 5 [58880/60000 ( 98%)]  Loss: 0.002234: 100%|██████████| 469/469 [00:01<00:00, 271.66it/s]\n",
      "Train Epoch: 6 [58880/60000 ( 98%)]  Loss: 0.000692: 100%|██████████| 469/469 [00:01<00:00, 240.74it/s]\n",
      "Train Epoch: 7 [58880/60000 ( 98%)]  Loss: 0.000634: 100%|██████████| 469/469 [00:01<00:00, 250.36it/s]\n",
      "Train Epoch: 8 [58880/60000 ( 98%)]  Loss: 0.005515: 100%|██████████| 469/469 [00:01<00:00, 253.42it/s]\n",
      "Train Epoch: 9 [58880/60000 ( 98%)]  Loss: 0.002394: 100%|██████████| 469/469 [00:02<00:00, 210.76it/s]\n",
      "Train Epoch: 10 [58880/60000 ( 98%)]  Loss: 0.001062: 100%|██████████| 469/469 [00:01<00:00, 265.74it/s]\n",
      "Train Epoch: 11 [58880/60000 ( 98%)]  Loss: 0.000053: 100%|██████████| 469/469 [00:01<00:00, 291.88it/s]\n",
      "Train Epoch: 12 [58880/60000 ( 98%)]  Loss: 0.005948: 100%|██████████| 469/469 [00:02<00:00, 203.99it/s]\n",
      "Train Epoch: 13 [58880/60000 ( 98%)]  Loss: 0.002456: 100%|██████████| 469/469 [00:02<00:00, 211.86it/s]\n",
      "Train Epoch: 14 [58880/60000 ( 98%)]  Loss: 0.002715: 100%|██████████| 469/469 [00:01<00:00, 301.39it/s]\n",
      "Train Epoch: 15 [58880/60000 ( 98%)]  Loss: 0.032687: 100%|██████████| 469/469 [00:01<00:00, 276.63it/s]\n",
      "Train Epoch: 16 [58880/60000 ( 98%)]  Loss: 0.006609: 100%|██████████| 469/469 [00:01<00:00, 286.74it/s]\n",
      "Train Epoch: 17 [58880/60000 ( 98%)]  Loss: 0.000852: 100%|██████████| 469/469 [00:01<00:00, 266.72it/s]\n",
      "Train Epoch: 18 [58880/60000 ( 98%)]  Loss: 0.008026: 100%|██████████| 469/469 [00:02<00:00, 227.23it/s]\n",
      "Train Epoch: 19 [58880/60000 ( 98%)]  Loss: 0.004844: 100%|██████████| 469/469 [00:01<00:00, 240.98it/s]\n",
      "Train Epoch: 20 [58880/60000 ( 98%)]  Loss: 0.007753: 100%|██████████| 469/469 [00:02<00:00, 230.79it/s]\n",
      "Train Epoch: 21 [58880/60000 ( 98%)]  Loss: 0.003017: 100%|██████████| 469/469 [00:01<00:00, 244.69it/s]\n",
      "Train Epoch: 22 [58880/60000 ( 98%)]  Loss: 0.004288: 100%|██████████| 469/469 [00:01<00:00, 241.12it/s]\n",
      "Train Epoch: 23 [58880/60000 ( 98%)]  Loss: 0.000303: 100%|██████████| 469/469 [00:01<00:00, 241.95it/s]\n",
      "Train Epoch: 24 [58880/60000 ( 98%)]  Loss: 0.010875: 100%|██████████| 469/469 [00:02<00:00, 231.28it/s]\n",
      "Train Epoch: 25 [58880/60000 ( 98%)]  Loss: 0.013587: 100%|██████████| 469/469 [00:01<00:00, 241.82it/s]\n",
      "Train Epoch: 26 [58880/60000 ( 98%)]  Loss: 0.001347: 100%|██████████| 469/469 [00:02<00:00, 229.82it/s]\n",
      "Train Epoch: 27 [58880/60000 ( 98%)]  Loss: 0.000314: 100%|██████████| 469/469 [00:01<00:00, 249.50it/s]\n",
      "Train Epoch: 28 [58880/60000 ( 98%)]  Loss: 0.015471: 100%|██████████| 469/469 [00:02<00:00, 201.74it/s]\n",
      "Train Epoch: 29 [58880/60000 ( 98%)]  Loss: 0.002705: 100%|██████████| 469/469 [00:01<00:00, 266.62it/s]\n",
      "Train Epoch: 30 [58880/60000 ( 98%)]  Loss: 0.000355: 100%|██████████| 469/469 [00:01<00:00, 272.13it/s]\n",
      "Train Epoch: 31 [58880/60000 ( 98%)]  Loss: 0.014142: 100%|██████████| 469/469 [00:01<00:00, 249.91it/s]\n",
      "Train Epoch: 32 [58880/60000 ( 98%)]  Loss: 0.000504: 100%|██████████| 469/469 [00:01<00:00, 236.96it/s]\n",
      "Train Epoch: 33 [58880/60000 ( 98%)]  Loss: 0.000213: 100%|██████████| 469/469 [00:01<00:00, 235.60it/s]\n",
      "Train Epoch: 34 [58880/60000 ( 98%)]  Loss: 0.004206: 100%|██████████| 469/469 [00:01<00:00, 238.16it/s]\n",
      "Train Epoch: 35 [58880/60000 ( 98%)]  Loss: 0.017878: 100%|██████████| 469/469 [00:02<00:00, 221.34it/s]\n",
      "Train Epoch: 36 [58880/60000 ( 98%)]  Loss: 0.030235: 100%|██████████| 469/469 [00:01<00:00, 249.88it/s]\n",
      "Train Epoch: 37 [58880/60000 ( 98%)]  Loss: 0.000328: 100%|██████████| 469/469 [00:02<00:00, 217.77it/s]\n",
      "Train Epoch: 38 [58880/60000 ( 98%)]  Loss: 0.001389: 100%|██████████| 469/469 [00:02<00:00, 215.58it/s]\n",
      "Train Epoch: 39 [58880/60000 ( 98%)]  Loss: 0.001892: 100%|██████████| 469/469 [00:02<00:00, 218.98it/s]\n",
      "Train Epoch: 40 [58880/60000 ( 98%)]  Loss: 0.001607: 100%|██████████| 469/469 [00:02<00:00, 223.13it/s]\n",
      "Train Epoch: 41 [58880/60000 ( 98%)]  Loss: 0.005892: 100%|██████████| 469/469 [00:01<00:00, 238.39it/s]\n",
      "Train Epoch: 42 [58880/60000 ( 98%)]  Loss: 0.053948: 100%|██████████| 469/469 [00:01<00:00, 260.43it/s]\n",
      "Train Epoch: 43 [58880/60000 ( 98%)]  Loss: 0.000289: 100%|██████████| 469/469 [00:01<00:00, 262.87it/s]\n",
      "Train Epoch: 44 [58880/60000 ( 98%)]  Loss: 0.010373: 100%|██████████| 469/469 [00:01<00:00, 257.40it/s]\n",
      "Train Epoch: 45 [58880/60000 ( 98%)]  Loss: 0.002542: 100%|██████████| 469/469 [00:01<00:00, 239.84it/s]\n",
      "Train Epoch: 46 [58880/60000 ( 98%)]  Loss: 0.000270: 100%|██████████| 469/469 [00:01<00:00, 252.10it/s]\n",
      "Train Epoch: 47 [58880/60000 ( 98%)]  Loss: 0.000114: 100%|██████████| 469/469 [00:01<00:00, 267.34it/s]\n",
      "Train Epoch: 48 [58880/60000 ( 98%)]  Loss: 0.000180: 100%|██████████| 469/469 [00:01<00:00, 259.37it/s]\n",
      "Train Epoch: 49 [58880/60000 ( 98%)]  Loss: 0.007582: 100%|██████████| 469/469 [00:01<00:00, 268.97it/s]\n",
      "Train Epoch: 50 [58880/60000 ( 98%)]  Loss: 0.005097: 100%|██████████| 469/469 [00:01<00:00, 262.82it/s]\n",
      "Train Epoch: 51 [58880/60000 ( 98%)]  Loss: 0.000698: 100%|██████████| 469/469 [00:01<00:00, 264.30it/s]\n",
      "Train Epoch: 52 [58880/60000 ( 98%)]  Loss: 0.001086: 100%|██████████| 469/469 [00:01<00:00, 261.43it/s]\n",
      "Train Epoch: 53 [58880/60000 ( 98%)]  Loss: 0.001103: 100%|██████████| 469/469 [00:01<00:00, 265.11it/s]\n",
      "Train Epoch: 54 [58880/60000 ( 98%)]  Loss: 0.009648: 100%|██████████| 469/469 [00:01<00:00, 259.33it/s]\n",
      "Train Epoch: 55 [58880/60000 ( 98%)]  Loss: 0.002221: 100%|██████████| 469/469 [00:01<00:00, 247.49it/s]\n",
      "Train Epoch: 56 [58880/60000 ( 98%)]  Loss: 0.013900: 100%|██████████| 469/469 [00:01<00:00, 243.34it/s]\n",
      "Train Epoch: 57 [58880/60000 ( 98%)]  Loss: 0.001830: 100%|██████████| 469/469 [00:01<00:00, 267.42it/s]\n",
      "Train Epoch: 58 [58880/60000 ( 98%)]  Loss: 0.007761: 100%|██████████| 469/469 [00:01<00:00, 243.54it/s]\n",
      "Train Epoch: 59 [58880/60000 ( 98%)]  Loss: 0.011300: 100%|██████████| 469/469 [00:01<00:00, 269.75it/s]\n",
      "Train Epoch: 60 [58880/60000 ( 98%)]  Loss: 0.046876: 100%|██████████| 469/469 [00:01<00:00, 308.56it/s]\n",
      "Train Epoch: 61 [58880/60000 ( 98%)]  Loss: 0.007312: 100%|██████████| 469/469 [00:01<00:00, 288.01it/s]\n",
      "Train Epoch: 62 [58880/60000 ( 98%)]  Loss: 0.002800: 100%|██████████| 469/469 [00:01<00:00, 301.19it/s]\n",
      "Train Epoch: 63 [58880/60000 ( 98%)]  Loss: 0.017751: 100%|██████████| 469/469 [00:01<00:00, 294.62it/s]\n",
      "Train Epoch: 64 [58880/60000 ( 98%)]  Loss: 0.028538: 100%|██████████| 469/469 [00:01<00:00, 290.56it/s]\n",
      "Train Epoch: 65 [58880/60000 ( 98%)]  Loss: 0.008500: 100%|██████████| 469/469 [00:02<00:00, 230.44it/s]\n",
      "Train Epoch: 66 [58880/60000 ( 98%)]  Loss: 0.000823: 100%|██████████| 469/469 [00:02<00:00, 222.44it/s]\n",
      "Train Epoch: 67 [58880/60000 ( 98%)]  Loss: 0.012817: 100%|██████████| 469/469 [00:01<00:00, 234.79it/s]\n",
      "Train Epoch: 68 [58880/60000 ( 98%)]  Loss: 0.009903: 100%|██████████| 469/469 [00:01<00:00, 264.33it/s]\n",
      "Train Epoch: 69 [58880/60000 ( 98%)]  Loss: 0.021952: 100%|██████████| 469/469 [00:01<00:00, 259.03it/s]\n",
      "Train Epoch: 70 [58880/60000 ( 98%)]  Loss: 0.010089: 100%|██████████| 469/469 [00:01<00:00, 256.55it/s]\n",
      "Train Epoch: 71 [58880/60000 ( 98%)]  Loss: 0.002361: 100%|██████████| 469/469 [00:01<00:00, 270.39it/s]\n",
      "Train Epoch: 72 [58880/60000 ( 98%)]  Loss: 0.007193: 100%|██████████| 469/469 [00:01<00:00, 247.74it/s]\n",
      "Train Epoch: 73 [58880/60000 ( 98%)]  Loss: 0.002563: 100%|██████████| 469/469 [00:01<00:00, 267.13it/s]\n",
      "Train Epoch: 74 [58880/60000 ( 98%)]  Loss: 0.002025: 100%|██████████| 469/469 [00:01<00:00, 261.67it/s]\n",
      "Train Epoch: 75 [58880/60000 ( 98%)]  Loss: 0.001461: 100%|██████████| 469/469 [00:01<00:00, 310.67it/s]\n",
      "Train Epoch: 76 [58880/60000 ( 98%)]  Loss: 0.000264: 100%|██████████| 469/469 [00:01<00:00, 296.01it/s]\n",
      "Train Epoch: 77 [58880/60000 ( 98%)]  Loss: 0.010738: 100%|██████████| 469/469 [00:01<00:00, 309.21it/s]\n",
      "Train Epoch: 78 [58880/60000 ( 98%)]  Loss: 0.056030: 100%|██████████| 469/469 [00:01<00:00, 292.33it/s]\n",
      "Train Epoch: 79 [58880/60000 ( 98%)]  Loss: 0.000515: 100%|██████████| 469/469 [00:01<00:00, 303.59it/s]\n",
      "Train Epoch: 80 [58880/60000 ( 98%)]  Loss: 0.011783: 100%|██████████| 469/469 [00:01<00:00, 289.17it/s]\n",
      "Train Epoch: 81 [58880/60000 ( 98%)]  Loss: 0.000969: 100%|██████████| 469/469 [00:01<00:00, 294.47it/s]\n",
      "Train Epoch: 82 [58880/60000 ( 98%)]  Loss: 0.001059: 100%|██████████| 469/469 [00:01<00:00, 276.56it/s]\n",
      "Train Epoch: 83 [58880/60000 ( 98%)]  Loss: 0.022977: 100%|██████████| 469/469 [00:01<00:00, 299.57it/s]\n",
      "Train Epoch: 84 [58880/60000 ( 98%)]  Loss: 0.003576: 100%|██████████| 469/469 [00:01<00:00, 306.76it/s]\n",
      "Train Epoch: 85 [58880/60000 ( 98%)]  Loss: 0.019466: 100%|██████████| 469/469 [00:01<00:00, 239.42it/s]\n",
      "Train Epoch: 86 [58880/60000 ( 98%)]  Loss: 0.001341: 100%|██████████| 469/469 [00:01<00:00, 251.99it/s]\n",
      "Train Epoch: 87 [58880/60000 ( 98%)]  Loss: 0.000572: 100%|██████████| 469/469 [00:01<00:00, 252.03it/s]\n",
      "Train Epoch: 88 [58880/60000 ( 98%)]  Loss: 0.032554: 100%|██████████| 469/469 [00:01<00:00, 253.75it/s]\n",
      "Train Epoch: 89 [58880/60000 ( 98%)]  Loss: 0.021054: 100%|██████████| 469/469 [00:01<00:00, 259.63it/s]\n",
      "Train Epoch: 90 [58880/60000 ( 98%)]  Loss: 0.003215: 100%|██████████| 469/469 [00:01<00:00, 268.85it/s]\n",
      "Train Epoch: 91 [58880/60000 ( 98%)]  Loss: 0.000213: 100%|██████████| 469/469 [00:01<00:00, 259.84it/s]\n",
      "Train Epoch: 92 [58880/60000 ( 98%)]  Loss: 0.000758: 100%|██████████| 469/469 [00:01<00:00, 259.44it/s]\n",
      "Train Epoch: 93 [58880/60000 ( 98%)]  Loss: 0.007794: 100%|██████████| 469/469 [00:01<00:00, 265.11it/s]\n",
      "Train Epoch: 94 [58880/60000 ( 98%)]  Loss: 0.010010: 100%|██████████| 469/469 [00:01<00:00, 261.59it/s]\n",
      "Train Epoch: 95 [58880/60000 ( 98%)]  Loss: 0.002490: 100%|██████████| 469/469 [00:01<00:00, 257.54it/s]\n",
      "Train Epoch: 96 [58880/60000 ( 98%)]  Loss: 0.001484: 100%|██████████| 469/469 [00:01<00:00, 234.50it/s]\n",
      "Train Epoch: 97 [58880/60000 ( 98%)]  Loss: 0.010848: 100%|██████████| 469/469 [00:02<00:00, 233.18it/s]\n",
      "Train Epoch: 98 [58880/60000 ( 98%)]  Loss: 0.001358: 100%|██████████| 469/469 [00:01<00:00, 288.82it/s]\n",
      "Train Epoch: 99 [58880/60000 ( 98%)]  Loss: 0.005073: 100%|██████████| 469/469 [00:01<00:00, 290.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Retraining\n",
    "optimizer.load_state_dict(initial_optimizer_state_dict) # Reset the optimizer\n",
    "\n",
    "model = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0729, Accuracy: 9828/10000 (98.28%). Total time = 0.34559106826782227\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight           | nonzeros =   97114 /  235200 ( 41.29%) | total_pruned =  138086 | shape = (300, 784)\n",
      "fc1.bias             | nonzeros =     300 /     300 (100.00%) | total_pruned =       0 | shape = (300,)\n",
      "fc2.weight           | nonzeros =   13315 /   30000 ( 44.38%) | total_pruned =   16685 | shape = (100, 300)\n",
      "fc2.bias             | nonzeros =     100 /     100 (100.00%) | total_pruned =       0 | shape = (100,)\n",
      "fc3.weight           | nonzeros =     723 /    1000 ( 72.30%) | total_pruned =     277 | shape = (10, 100)\n",
      "fc3.bias             | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 111562, pruned : 155048, total: 266610, Compression rate :       2.39x  ( 58.16% pruned)\n"
     ]
    }
   ],
   "source": [
    "print_nonzeros(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LUONG TU HOA VA SHARE WEIGHT\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import csc_matrix, csr_matrix #hai format de luu tru ma tran thu nham tinh toan duoc de dang do tiet kiem bo nho.\n",
    "\n",
    "def apply_weight_sharing(model, bits=5): # co toi da 2^5 = 32 cum cua kmeans\n",
    "    for module in model.children():\n",
    "        dev = module.weight.device\n",
    "        weight = module.weight.data.cpu().numpy()\n",
    "        shape = weight.shape\n",
    "        mat = csr_matrix(weight) if shape[0] < shape[1] else csc_matrix(weight)\n",
    "        min_ = min(mat.data)\n",
    "        max_ = max(mat.data)\n",
    "        space = np.linspace(min_, max_, num=2**bits)\n",
    "        kmeans = KMeans(n_clusters=len(space), init=space.reshape(-1,1), n_init=1, algorithm=\"full\")\n",
    "        kmeans.fit(mat.data.reshape(-1,1))\n",
    "        new_weight = kmeans.cluster_centers_[kmeans.labels_].reshape(-1) # share lại centroid vào các vị trí weight\n",
    "        mat.data = new_weight\n",
    "        module.weight.data = torch.from_numpy(mat.toarray()).to(dev)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maicg/anaconda3/envs/Face-insight/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1318: FutureWarning: algorithm='full' is deprecated, it will be removed in 1.3. Using 'lloyd' instead.\n",
      "  warnings.warn(\n",
      "/home/maicg/anaconda3/envs/Face-insight/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1318: FutureWarning: algorithm='full' is deprecated, it will be removed in 1.3. Using 'lloyd' instead.\n",
      "  warnings.warn(\n",
      "/home/maicg/anaconda3/envs/Face-insight/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1318: FutureWarning: algorithm='full' is deprecated, it will be removed in 1.3. Using 'lloyd' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (fc1): Linear(in_features=784, out_features=300, bias=True)\n",
       "  (fc2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_weight_sharing(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0714, Accuracy: 9831/10000 (98.31%). Total time = 0.36731457710266113\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight           | nonzeros =   97114 /  235200 ( 41.29%) | total_pruned =  138086 | shape = (300, 784)\n",
      "fc1.bias             | nonzeros =     300 /     300 (100.00%) | total_pruned =       0 | shape = (300,)\n",
      "fc2.weight           | nonzeros =   13315 /   30000 ( 44.38%) | total_pruned =   16685 | shape = (100, 300)\n",
      "fc2.bias             | nonzeros =     100 /     100 (100.00%) | total_pruned =       0 | shape = (100,)\n",
      "fc3.weight           | nonzeros =     723 /    1000 ( 72.30%) | total_pruned =     277 | shape = (10, 100)\n",
      "fc3.bias             | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 111562, pruned : 155048, total: 266610, Compression rate :       2.39x  ( 58.16% pruned)\n"
     ]
    }
   ],
   "source": [
    "print_nonzeros(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Face-insight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
